{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lceor_dh6wWy",
        "rSn-_bhXlc9v",
        "K7EA-ZHtl46S",
        "PDlU6ZKBmtD5",
        "7p3O-cwhm4iU",
        "ViiwWv3nnHBv",
        "hkoZjnmyn1-F",
        "dxd6AlcZkPKS",
        "cbnbcbfPoUyl",
        "v0NkbX7FuYYl",
        "ypszSO5zwFAj",
        "jmkXX55iwIZY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slibolt/ADS500B/blob/main/ADS502_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Import and Setup"
      ],
      "metadata": {
        "id": "lceor_dh6wWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "NMGwInzwj_Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "df_original = pd.read_csv(\"breast_cancer.csv\") #URL from raw github\n",
        "df_original.head()\n"
      ],
      "metadata": {
        "id": "csoHKuomkC4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data trim, per study \"best predictive accuracy obtained using one separating plane in the 3-D space of Worst Area, Worst Smoothness and Mean Texture.\"\n",
        "df_trim = df_original[['diagnosis', 'area_worst', 'smoothness_worst', 'texture_mean', 'symmetry_se', 'fractal_dimension_mean']]\n",
        "df_trim.head()\n"
      ],
      "metadata": {
        "id": "IR8sQcGwlKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert diagnosis into binary\n",
        "df_dummy = pd.get_dummies(df_trim['diagnosis'])\n",
        "#concatnate into dataframe\n",
        "df= pd.concat((df_dummy,df_trim), axis = 1)\n",
        "df = df.drop(['diagnosis'], axis = 1)\n",
        "df = df.drop(['B'], axis = 1)\n",
        "df['M'] = df['M'].astype(int)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cq4E9WW9lUSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['M'].dtype"
      ],
      "metadata": {
        "id": "TqoPgqu06G3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Data Information**"
      ],
      "metadata": {
        "id": "rSn-_bhXlc9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "B7c3biyZlbcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#are there duplicates?\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "CNqt-JlEliNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='M', data=df)"
      ],
      "metadata": {
        "id": "dxznpP-bluGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percentage of binary class\n",
        "print(\"percentage of each class\", df['M'].value_counts()/len(df)*100)"
      ],
      "metadata": {
        "id": "TGCf-iO-lyWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Quality Report**\n",
        "\n",
        "## **Continuous Features**"
      ],
      "metadata": {
        "id": "K7EA-ZHtl46S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identify continuous features\n",
        "conf = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "conf"
      ],
      "metadata": {
        "id": "BsUtdAU4l1Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get summary stats\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "pYCkHqxFmDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_quality_conf = pd.DataFrame({\n",
        "    'Feature': conf,\n",
        "    'Count': df.count().values,\n",
        "    'Missing Values': df.isnull().sum().values,\n",
        "    'Cardinality': df.nunique().values,\n",
        "    'Min': df.min().values,\n",
        "    '1st Quartile': df.quantile(0.25).values,\n",
        "    'Mean': df.mean().values,\n",
        "    'Median': df.median().values,\n",
        "    '3rd Quartile': df.quantile(0.75).values,\n",
        "    'Max': df.max().values,\n",
        "    'Standard Deviation': df.std().values,\n",
        "})\n",
        "data_quality_conf"
      ],
      "metadata": {
        "id": "5KX5j5lWmo1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**"
      ],
      "metadata": {
        "id": "PDlU6ZKBmtD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot histograms for numerical variables\n",
        "plt.style.use('ggplot')\n",
        "for column in conf:\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df[column], kde = True)\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yPTc6FeDmrfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot boxplots of all continuous features\n",
        "plt.style.use('ggplot')\n",
        "for column in conf:\n",
        "    if column != 'M':\n",
        "        plt.figure(figsize=(20, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.boxplot(x=df[column])\n",
        "        plt.title(f'Boxplot of {column}')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "tc3Z4K5Em0Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Analysis**"
      ],
      "metadata": {
        "id": "7p3O-cwhm4iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#correlations, all\n",
        "corr_matrix = df.corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "_vxz6Gipm3Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "heatmap = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, annot_kws={\"size\": 8})\n",
        "\n",
        "# Rotate the x and y labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ai-38IZlnAut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue =\"M\", height=3)"
      ],
      "metadata": {
        "id": "aUHgAqJBnC5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Scaling**\n"
      ],
      "metadata": {
        "id": "ViiwWv3nnHBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Isolate features (X) and target (y)\n",
        "X = df[['area_worst', 'smoothness_worst', 'texture_mean']]\n",
        "y = df['M']\n",
        "\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "JufeEi3CnFLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stratified K-Fold Partitioning**"
      ],
      "metadata": {
        "id": "uQfawvBAnT_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Initialize StratifiedKFold with 10 folds\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# get list of partitions\n",
        "def get_partitions(X, y):\n",
        "  partitions = []\n",
        "  # Performing stratified k-fold cross-validation\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "      X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "      partitions.append((X_train, X_test, y_train, y_test))\n",
        "  return partitions"
      ],
      "metadata": {
        "id": "vgUNy1bdnS7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizations**"
      ],
      "metadata": {
        "id": "s2s9oICSnZXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define color maps for visualization\n",
        "cmap_cv = plt.get_cmap('coolwarm')\n",
        "cmap_data = plt.get_cmap('tab10')\n",
        "\n",
        "# Define visualization function for cross-validation indices\n",
        "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
        "\n",
        "    #Create a plot for indices of a cross-validation object\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1  # Testing set\n",
        "        indices[tr] = 0  # Training set\n",
        "        ax.scatter(range(len(indices)), [ii + 0.5] * len(indices), c=indices, marker=\"_\", lw=lw, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
        "\n",
        "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data)\n",
        "    yticklabels = list(range(n_splits)) + [\"class\"]\n",
        "    ax.set(yticks=np.arange(n_splits + 1) + 0.5, yticklabels=yticklabels, xlabel=\"Sample index\", ylabel=\"CV iteration\", ylim=[n_splits + 1.2, -0.2], xlim=[0, len(X)])\n",
        "    ax.set_title(\"Cross-Validation Splits\", fontsize=15)\n",
        "    return ax\n",
        "\n",
        "# Creating a plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "plot_cv_indices(skf, X_scaled, y, ax, n_splits=10)\n",
        "plt.show()\n",
        "\n",
        "# Plotting fold distribution\n",
        "def plot_fold_distribution(cv, X, y, ax):\n",
        "    fold_sizes = [np.sum(y.iloc[tt] == 1) for _, tt in cv.split(X, y)]\n",
        "    class_0 = [np.sum(y.iloc[tt] == 0) for _, tt in cv.split(X, y)]\n",
        "    class_1 = [np.sum(y.iloc[tt] == 1) for _, tt in cv.split(X, y)]\n",
        "\n",
        "    df_fold = pd.DataFrame({'Fold': list(range(len(fold_sizes))), 'Class 0': class_0, 'Class 1': class_1})\n",
        "    df_fold.set_index('Fold').plot(kind='bar', ax=ax)\n",
        "    ax.set_xlabel('Fold')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title('Distribution of Classes Across Folds')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_fold_distribution(skf, X_scaled, y, ax)\n",
        "plt.show()\n",
        "\n",
        "# Plotting class distribution heatmap\n",
        "def plot_class_distribution_heatmap(cv, X, y, ax):\n",
        "    fold_class_distribution = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        fold_class_distribution.append(np.bincount(y.iloc[test_idx], minlength=2))\n",
        "\n",
        "    df_class_dist = pd.DataFrame(fold_class_distribution, columns=['Class 0', 'Class 1'])\n",
        "    sns.heatmap(df_class_dist, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
        "    ax.set_xlabel('Class')\n",
        "    ax.set_ylabel('Fold')\n",
        "    ax.set_title('Class Distribution Across Folds')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_class_distribution_heatmap(skf, X_scaled, y, ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ErYDIEIVnY5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "s5pHLzQ8nzra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up Model Evaluation Table\n",
        "model_evaluation_table = {\n",
        "    'Evaluation Measure': ['Accuracy', 'Error Rate', 'Recall', 'Precision', 'F1 Score', 'ROC AUC'],\n",
        "      'Logistic Regression (Baseline)': [0] * 6,  # Initialize with placeholder values (replace 0 with actual metrics later)\n",
        "    'Neural Network': [0] * 6,\n",
        "    'Random Forest': [0] * 6,\n",
        "    'Naive Bayes': [0] * 6\n",
        "}\n",
        "model_evaluation_df = pd.DataFrame(model_evaluation_table)\n",
        "model_evaluation_df"
      ],
      "metadata": {
        "id": "xwaqcRqQVAJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  format metric\n",
        "def format_metric(metric):\n",
        "    return f'{metric:.4f}'\n"
      ],
      "metadata": {
        "id": "hLElx3oDnimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model (Logistic Regression)"
      ],
      "metadata": {
        "id": "hkoZjnmyn1-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create table to store metrics for cross validation\n",
        "metrics_logreg = pd.DataFrame(columns =['Fold','Accuracy', 'Error Rate', 'Recall', 'Precision', 'F1 Score', 'ROC AUC'])"
      ],
      "metadata": {
        "id": "DGPR11LfY5Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store metrics\n",
        "metrics_list = []\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "#initialize fold counter\n",
        "fold_counter = 1\n",
        "\n",
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    # Initializing the logistic regression model\n",
        "    lg_model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "    # Train the model on the training data\n",
        "    lg_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = lg_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    error_rate = 1 - accuracy\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    y_prob = lg_model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "\n",
        "     # Collect metrics for this fold\n",
        "    metrics_list.append({\n",
        "        'Fold': fold_counter,\n",
        "        'Accuracy': accuracy,\n",
        "        'Error Rate': error_rate,\n",
        "        'Recall': recall,  # Specificity should be calculated differently, using recall here for demonstration\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "\n",
        "    # # Increment fold counter\n",
        "    fold_counter += 1\n",
        "\n",
        "metrics_logreg = pd.DataFrame(metrics_list)\n",
        "metrics_logreg\n"
      ],
      "metadata": {
        "id": "5xLPj2AGn8-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get average metrics across folds\n",
        "mean_metrics_logreg = metrics_logreg.loc[:, metrics_logreg.columns != 'Fold'].mean()\n",
        "mean_metrics_logreg"
      ],
      "metadata": {
        "id": "1ydKcOwxeQ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update comparison table\n",
        "model_evaluation_df['Logistic Regression (Baseline)'] = [\n",
        "    format_metric(mean_metrics_logreg['Accuracy']),\n",
        "    format_metric(mean_metrics_logreg['Error Rate']),\n",
        "    format_metric(mean_metrics_logreg['Recall']),\n",
        "    format_metric(mean_metrics_logreg['Precision']),\n",
        "    format_metric(mean_metrics_logreg['F1 Score']),\n",
        "    format_metric(mean_metrics_logreg['ROC AUC'])\n",
        "]\n",
        "model_evaluation_df"
      ],
      "metadata": {
        "id": "04ivuwv2hVCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "cbnbcbfPoUyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create table to store metrics for Neural Network\n",
        "metrics_nn = pd.DataFrame(columns =['Fold','Accuracy', 'Error Rate', 'Recall', 'Precision', 'F1 Score', 'ROC AUC'])"
      ],
      "metadata": {
        "id": "3zVV4mI5lRpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "metrics_list_nn = []\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "#initialize Fold Counter\n",
        "fold_counter = 1"
      ],
      "metadata": {
        "id": "jyKz_Yh40c-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    clf = MLPClassifier(solver='lbfgs', max_iter=800, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=62)\n",
        "\n",
        "    # train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "     #Collect Metrics for this fold:\n",
        "    metrics_list_nn.append({\n",
        "        'Fold': fold_counter,\n",
        "        'Accuracy': accuracy,\n",
        "        'Error Rate': error_rate,\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "\n",
        "    # Increment fold counter\n",
        "    fold_counter += 1\n",
        "\n",
        "metrics_nn = pd.DataFrame(metrics_list_nn)\n",
        "metrics_nn"
      ],
      "metadata": {
        "id": "dW7Nby6S0ZPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get average metrics across folds\n",
        "mean_metrics_nn = metrics_nn.loc[:, metrics_logreg.columns != 'Fold'].mean()"
      ],
      "metadata": {
        "id": "i745Bo-tnOLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update comparison table\n",
        "model_evaluation_df['Neural Network'] = [\n",
        "    format_metric(mean_metrics_nn['Accuracy']),\n",
        "    format_metric(mean_metrics_nn['Error Rate']),\n",
        "    format_metric(mean_metrics_nn['Recall']),\n",
        "    format_metric(mean_metrics_nn['Precision']),\n",
        "    format_metric(mean_metrics_nn['F1 Score']),\n",
        "    format_metric(mean_metrics_nn['ROC AUC'])\n",
        "]\n",
        "model_evaluation_df"
      ],
      "metadata": {
        "id": "X01gWTGPnV-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest**"
      ],
      "metadata": {
        "id": "v0NkbX7FuYYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a table to store metrics for cross validation\n",
        "metrics_rf = pd.DataFrame(columns =['Fold','Accuracy', 'Error Rate', 'Recall', 'Precision', 'F1 Score', 'ROC AUC'])"
      ],
      "metadata": {
        "id": "xH5dG-LDoPIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "metrics_list_rf = []\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "#initialize fold counter\n",
        "fold_counter = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "-FsPJR8roPO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    #initializing random forest\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    # Train the model on the training data\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    error_rate = 1 - accuracy\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "\n",
        "     # Collect metrics for this fold\n",
        "    metrics_list_rf.append({\n",
        "        'Fold': fold_counter,\n",
        "        'Accuracy': accuracy,\n",
        "        'Error Rate': error_rate,\n",
        "        'Recall': recall,  # Specificity should be calculated differently, using recall here for demonstration\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "\n",
        "    # Increment fold counter\n",
        "    fold_counter += 1\n",
        "\n",
        "metrics_rf = pd.DataFrame(metrics_list_rf)\n",
        "metrics_rf\n"
      ],
      "metadata": {
        "id": "QprF4hesowOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get average metrics accross folds\n",
        "mean_metrics_rf = metrics_rf.mean()"
      ],
      "metadata": {
        "id": "0PmneoZ5qiyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update comparison table\n",
        "model_evaluation_df['Random Forest'] = [\n",
        "    format_metric(mean_metrics_rf['Accuracy']),\n",
        "    format_metric(mean_metrics_rf['Error Rate']),\n",
        "    format_metric(mean_metrics_rf['Recall']),\n",
        "    format_metric(mean_metrics_rf['Precision']),\n",
        "    format_metric(mean_metrics_rf['F1 Score']),\n",
        "    format_metric(mean_metrics_rf['ROC AUC'])\n",
        "]\n",
        "model_evaluation_df"
      ],
      "metadata": {
        "id": "L6VWJySRsVSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes**"
      ],
      "metadata": {
        "id": "cnECzk1RwMzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create table for nb metrics\n",
        "metrics_nb = pd.DataFrame(columns =['Fold','Accuracy', 'Error Rate', 'Recall', 'Precision', 'F1 Score', 'ROC AUC'])"
      ],
      "metadata": {
        "id": "laF36JrJ2ZlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize list to store metrics\n",
        "metrics_list_nb = []\n",
        "accuracy_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "roc_aucs = []\n",
        "\n",
        "#initialize fold counter\n",
        "fold_counter = 1"
      ],
      "metadata": {
        "id": "iI7Qd4IW2Zn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in get_partitions(X_scaled, y):\n",
        "    # Unpack partition into constituent variables\n",
        "    (X_train, X_test, y_train, y_test) = part\n",
        "\n",
        "    # Initialize Naive Bayes model\n",
        "    nb_model = GaussianNB()\n",
        "\n",
        "    # Train the model on the training data\n",
        "    nb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    error_rate = 1 - accuracy\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_aucs.append(roc_auc)\n",
        "\n",
        "    # Collect metrics for this fold\n",
        "    metrics_list_nb.append({\n",
        "        'Fold': fold_counter,\n",
        "        'Accuracy': accuracy,\n",
        "        'Error Rate': error_rate,\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "\n",
        "    # Increment fold counter\n",
        "    fold_counter += 1\n",
        "\n",
        "metrics_nb = pd.DataFrame(metrics_list_nb)\n",
        "metrics_nb"
      ],
      "metadata": {
        "id": "PxErHgFx2Zqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get average metrics across folds\n",
        "mean_metrics_nb = metrics_nb.mean()"
      ],
      "metadata": {
        "id": "Nf1if7tH3qwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update comparison table\n",
        "model_evaluation_df['Naive Bayes'] = [\n",
        "    format_metric(mean_metrics_nb['Accuracy']),\n",
        "    format_metric(mean_metrics_nb['Error Rate']),\n",
        "    format_metric(mean_metrics_nb['Recall']),\n",
        "    format_metric(mean_metrics_nb['Precision']),\n",
        "    format_metric(mean_metrics_nb['F1 Score']),\n",
        "    format_metric(mean_metrics_nb['ROC AUC'])\n",
        "]\n",
        "model_evaluation_df"
      ],
      "metadata": {
        "id": "NiSRFME53yBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison"
      ],
      "metadata": {
        "id": "mXkofnSRvY9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the ROC curves for all models\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for model, name in zip([lg_model, clf, rf_model, nb_model], ['Logistic Regression','Neural Network', 'Random Forest', 'Naive Bayes']):\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for train_index, test_index in skf.split(X_scaled, y):\n",
        "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_test)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "\n",
        "    mean_tpr /= skf.get_n_splits()\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'{name} (area = {mean_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUY014P-vTst"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}